{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building your first AzureML Spark web service\n",
    "\n",
    "In this tutorial, we will walk you through loading a dataset, exploring\n",
    "its features, training a model on the dataset, and then publishing a\n",
    "realtime scoring API for the model.\n",
    "\n",
    "First, let's read in the Boston Housing Price dataset. This dataset is publicly available at https://archive.ics.uci.edu/ml/datasets/Housing. We have placed a copy in your azureml/datasets folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing Azure ML API SDK functionality. The SDK is installed implicitly with the latest\n",
    "# version of the CLI in your default python environment\n",
    "from azure.ml.api.schema.dataTypes import DataTypes\n",
    "from azure.ml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azure.ml.api.realtime.services import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+-----+----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|TAX|PTRATIO|LSTAT|MEDV|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+-----+----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|296|   15.3| 4.98|24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|242|   17.8| 9.14|21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8| 4.03|34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|222|   18.7| 2.94|33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|222|   18.7| 5.33|36.2|\n",
      "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7| 5.21|28.7|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311|   15.2|12.43|22.9|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2|19.15|27.1|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|29.93|16.5|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2| 17.1|18.9|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|311|   15.2|20.45|15.0|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|311|   15.2|13.27|18.9|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|311|   15.2|15.71|21.7|\n",
      "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|307|   21.0| 8.26|20.4|\n",
      "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|307|   21.0|10.26|18.2|\n",
      "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|307|   21.0| 8.47|19.9|\n",
      "|1.05393| 0.0| 8.14|   0|0.538|5.935| 29.3|4.4986|  4|307|   21.0| 6.58|23.1|\n",
      "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|307|   21.0|14.67|17.5|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|307|   21.0|11.69|20.2|\n",
      "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|307|   21.0|11.28|18.2|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- CRIM: double (nullable = true)\n",
      " |-- ZN: double (nullable = true)\n",
      " |-- INDUS: double (nullable = true)\n",
      " |-- CHAS: integer (nullable = true)\n",
      " |-- NOX: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- DIS: double (nullable = true)\n",
      " |-- RAD: integer (nullable = true)\n",
      " |-- TAX: integer (nullable = true)\n",
      " |-- PTRATIO: double (nullable = true)\n",
      " |-- LSTAT: double (nullable = true)\n",
      " |-- MEDV: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the housing price dataset\n",
    "df2 = spark.read.csv(\"datasets/housing.csv\", header=True, inferSchema=True)\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train your model\n",
    "\n",
    "Using Spark's ML library, we can train a gradient boosted tree regressor for our data to produce a model that can predict median values of houses in Boston. Once we have trained the model, we can then evaluate it for quality using the root mean squared error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train a boosted decision tree regressor\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "import numpy as np\n",
    "formula = RFormula(formula=\"MEDV~.\")\n",
    "gbt = GBTRegressor()\n",
    "pipeline = Pipeline(stages=[formula, gbt]).fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 error = 0.977613898997\n"
     ]
    }
   ],
   "source": [
    "# Evaluate scores\n",
    "scores = pipeline.transform(df2)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "print \"R^2 error =\", RegressionEvaluator(metricName=\"r2\").evaluate(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save your model and schema\n",
    "\n",
    "Once you have a model that performs well, you can package it into a scoring service. To prepare for this, save your model and dataset schema locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "pipeline.write().overwrite().save(\"housing.model\")\n",
    "print \"Model saved\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Authoring a Realtime Web Service\n",
    "\n",
    "In this section, we show you how to author a realtime web service that scores the model you saved above. \n",
    "\n",
    "### 1. Define ```init``` and ```run```\n",
    "\n",
    "We start by defining our ```init``` and ```run``` functions in the cell below. \n",
    "\n",
    "The ```init``` function initializes your web service, loading in any data or models that you need to score your inputs. In the example below, we load in the trained model and the schema of our dataset.\n",
    "\n",
    "The ```run``` function defines what is executed on a scoring call. In our simple example, we simply load in the json input as a data frame, and run our pipeline on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#%%save_file -f driver.py\n",
    "# User written init function should mainly focus on loading the model(s) now. \n",
    "# Schema loading is done in generated code\n",
    "def init():\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load(\"housing.model\")\n",
    "\n",
    "\n",
    "# Run method now takes actual objects (dataframes, numpy arrays, other types) as arguments\n",
    "# so the user no longer needs to worry about HTTP body parsing to object(s), \n",
    "# the generated code does that, if schema is generated\n",
    "def run(input_df):\n",
    "    score = pipeline.transform(input_df)\n",
    "    return score.collect()[0]['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To generate schema for inputs (and outputs for rich swagger), users should define a map of input name -> input sample\n",
    "# where input name needs to match exactly with the names of arguments for the run function, and for samples the\n",
    "# user is advised to use the data structures he likely already created and used for testing the model after training\n",
    "inputs = {\"input_df\": SampleDefinition(DataTypes.SPARK, df2.drop(\"MEDV\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing inputs\n",
      "processing init function\n",
      "processing run function\n",
      "setting up output directory\n",
      "Done setting up output directory, available here output_20170622202541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output_20170622202541'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we put it all together by calling prepare with the init, run and inputs (and/or outputs) definitions.\n",
    "# This will create a folder named output_{timestamp} in the current working directory (by default, or can use the\n",
    "# drop_folder param to override that) and inside it it will generate the driver program - main.py and schema file -\n",
    "# service_schema.json (if inputs or outputs definitions are specified).\n",
    "# The user can then use the generated driver with the -f option in the CLI and schema for -s when publishing a service\n",
    "prepare(run_func=run, init_func=init, input_types=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Test ```init``` and ```run```\n",
    "\n",
    "We can then test the ```init``` and ```run``` functions right here in the notebook, before we decide to actually publish a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6874685183\n"
     ]
    }
   ],
   "source": [
    "# Here, if publisher wants to test the init and run functions like before in the notebook, I had to create a DF for\n",
    "# the input instead of a string since an data frame is expected now. \n",
    "input_data = [[0.00632, 18.0, 2.31, 0, 0.538, 6.575, 65.2, 4.09, 1, 296, 15.3, 4.98, 24.0]]\n",
    "df = spark.createDataFrame(input_data, [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"])\n",
    "init()\n",
    "print(run(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Save your web service definition\n",
    "\n",
    "Go back to the cell where you defined your ```init``` and ```run``` functions, uncomment the magic in the first line (```#%%save_file -f testing.py```), and run the cell again. This will save the contents of the cell to a local file with the name supplied to the ```-f``` argument.\n",
    "\n",
    "### 4. Use the Azure Machine Learning CLI to deploy and manage your web services\n",
    "\n",
    "Switch to a bash shell, and run the following commands to deploy your service locally:\n",
    "```\n",
    "cd ~home/azuremluser/notebooks/azureml/realtime/\n",
    "az ml env local\n",
    "az ml service create realtime -f main.py -m ../housing.model -s service_schema.json -n mytestapp1 -r spark-py -v\n",
    "\n",
    "```\n",
    "### 5. Test the Web Service\n",
    "Use the below command to get the usage help with sample data input:\n",
    "\n",
    "```\n",
    "az ml service \n",
    "```\n",
    "\n",
    "### 6. Get the Swagger document\n",
    "\n",
    "```\n",
    "curl http://127.0.0.1:<portNumber>/swagger.json \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 Spark - local",
   "language": "python",
   "name": "spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
