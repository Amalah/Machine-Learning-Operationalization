{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How to create a batch web service for a Spark model on Azure\n",
    "\n",
    "Before running the tutorial, you must configure your DSVM as specified in the README on the [Machine Learing Operationaliztion](https://aka.ms/o16ncli) GitHub repo. If you have previously configured your DSVM, you may want to check the GitHub repo to ensure that you are using the most recent instructions.\n",
    "\n",
    "In the tutorial you will use [Apache Spark](http://spark.apache.org/) to create a model that uses a Logistic Regression learner to predict food inspection results. To do this, you will call the Spark Python API ([PySpark](http://spark.apache.org/docs/0.9.0/python-programming-guide.html)) to load a dataset, train a model using the dataset, and publish a batch scoring API for the model.\n",
    "\n",
    "You then use the Azure CLI to operationalize the model and to call the web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data\n",
    "\n",
    "The tutorial uses the *Food Inspections Data Set* which contains the results of food inspections that were conducted in Chicago. To facilitate this tutorial, we have placed a copy of the data in the ```azureml/datasets``` folder. The original dataset is available from the [City of Chicago data portal](https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import the relevant PySpark bindings\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Parse the food inspections dataset and create numerical labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inspections = spark.read.csv(\"../datasets/food_inspections1.csv\",mode='DROPMALFORMED',inferSchema=False)\n",
    "\n",
    "schema = StructType([StructField(\"id\", IntegerType(), False), \n",
    "                     StructField(\"name\", StringType(), False), \n",
    "                     StructField(\"results\", StringType(), False), \n",
    "                     StructField(\"violations\", StringType(), True)])\n",
    "\n",
    "df = sqlContext.createDataFrame(inspections.rdd.map(lambda l: (int(l[0]), l[1], l[12], l[13] if l[13] else '')), schema) \n",
    "df.registerTempTable('CountResults')\n",
    "\n",
    "def labelForResults(s):\n",
    "    if s == 'Fail':\n",
    "        return 0.0\n",
    "    elif s == 'Pass w/ Conditions' or s == 'Pass':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return -1.0\n",
    "    \n",
    "label = UserDefinedFunction(labelForResults, DoubleType())\n",
    "labeledData = df.select(label(df.results).alias('label'), df.violations).where('label >= 0')\n",
    "labeledData.write.format('parquet').mode('overwrite').save('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create and save the model\n",
    "Next, train a logistic regression model to predict inspection results. The following code tokenizes each \"violations\" string to get the individual words in each string. It then uses a HashingTF to convert each set of tokens into a feature vector which is passed to the logistic regression algorithm to construct a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(labeledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, you save the model to use when deploying the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save(\"food_inspection.model\")\n",
    "print \"Model saved\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a Batch Web Service\n",
    "\n",
    "In this section, you will create and deploy a batch webservice that will make predictions on given data using the model that you trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create a prediction script \n",
    "\n",
    "Your goal is to create a web service that you can call to make predictions based on the input data. To create a web service using the model you saved, you start by authoring a function to do the scoring.\n",
    "\n",
    "In the provided sample, the function takes a Spark Dataframe as its input-data argument, uses the model specified by the user as model input, and makes predictions on the data by running the model. The function then saves the predictions as a parquet file to the path provided through the output-data argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Azure ML API SDK. The SDK is installed implicitly with the latest\n",
    "# version of the CLI in your default python environment\n",
    "from azure.ml.api.schema.dataTypes import DataTypes\n",
    "from azure.ml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azure.ml.api.batch.batch_handler import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def run(input_data, trained_model, output_data):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    sqlContext = SQLContext.getOrCreate(sc)\n",
    "    \n",
    "    schema = StructType([StructField(\"id\", IntegerType(), False),\n",
    "                     StructField(\"name\", StringType(), False),\n",
    "                     StructField(\"results\", StringType(), False),\n",
    "                     StructField(\"violations\", StringType(), True)])\n",
    "\n",
    "    testDf = sqlContext.createDataFrame(input_data.rdd.map(lambda l: (int(l[0]), l[1], l[12], l[13] if l[13] else '')), schema).where(\"results = 'Fail' OR results = 'Pass' OR results = 'Pass w/ Conditions'\")\n",
    "    \n",
    "    predictionsDf = trained_model.transform(testDf)\n",
    "    predictionsDf.write.format(\"parquet\").mode('overwrite').save(str(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a schema file \n",
    "\n",
    "To generate a schema for the inputs and models, you define a map of input names to tuple of input sample data and boolean, where the boolean signifies whether or not to expect headers for the data file. Outputs currently only map to a standard sample datatype. Parameters are used for any non-file inputs that the function expects, and also map to a standard sample type. The names for the inputs, outputs, and parameters must match exactly with the names of the arguments for the run function. For samples use the data structures you created and used for testing the model after training. Lastly, you can list any dependencies that your function has that are not already provided in the default environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the driver and schema files\n",
    "\n",
    "Finally, we put all of this together by calling the prepare function with the run, and inputs (and/or outputs) definitions.\n",
    "\n",
    "This creates two files in the current working directory, the driver program named *service_driver.py*, and a schema file named *batch_schema_{timestamp}*.\n",
    "\n",
    "The function outputs a command to call using the az ml cli to publish the created batch service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "az ml service create batch -f service_driver.py -n batch_score --in=--input-data --in=--trained-model --out=--output-data -d batch_schema_20170630232457.json\n"
     ]
    }
   ],
   "source": [
    "inputs = {'input_data': (SampleDefinition(DataTypes.SPARK, labeledData), True),\n",
    "          'trained_model': (SampleDefinition(DataTypes.SPARK, labeledData), True)}\n",
    "outputs = {'output_data': SampleDefinition(DataTypes.STANDARD, 'output.parquet')}\n",
    "parameters = {}\n",
    "dependencies = []\n",
    "\n",
    "prepare(run_func=run, inputs=inputs, outputs=outputs, parameters=parameters, dependencies=dependencies, service_name='batch_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Use the CLI to deploy and manage your batch web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can deploy an operationalized model as a web service locally and to a cluster.\n",
    "\n",
    "Open an SSH session to your DSVM and change to the folder notebooks/azureml/batch.\n",
    "\n",
    "```\n",
    "cd ~/notebooks/azureml/batch\n",
    "```\n",
    "\n",
    "#### Deploy to local machine\n",
    "\n",
    "To create the batch web service locally on the DSVM, set your CLI environment to run in local mode.\n",
    "\n",
    "```\n",
    "az ml env local\n",
    "```\n",
    "\n",
    "The following command creates a web service local to the DSVM. It reads the model from local storage and specifies that the output is written to local storage.\n",
    "\n",
    "```\n",
    "az ml service create batch -f batch_score.py -n batchwebservice --in=--input-data --in=--trained-model:food_inspection.model --out=--output-data:food_inspection_predictions.parquet\n",
    "```\n",
    "\n",
    "Once the web service is successfully created, the following command runs a job against the web service:\n",
    "\n",
    "```\n",
    "az ml service run batch -n samplebatch --out=--output-data:output.parquet -w\n",
    "```\n",
    "\n",
    "#### Deploy to a cluster\n",
    "\n",
    "In the following example, the input data is stored remotely. When you create the batch web service, you can use data that is stored in a private blob in Azure storage. When using this scenario, the blob must be in the storage account that was created during your environment setup and that setup must be the active one in your az ml CLI environment. When you create the service the CLI uses the credentials stored in your environment to retrieve the data.\n",
    "\n",
    "Download the *food_inspections2.csv* data file and upload it to a container in storage account that was created when you set up your environment.\n",
    "\n",
    "To find the name of the storage account, open your *.amlenvrc* file and find the AML_STORAGE_ACCT_NAME variable.\n",
    "\n",
    "To create the batch web service locally on the DSVM, set your CLI environment to run in local mode.\n",
    "```\n",
    "az ml env cluster\n",
    "```\n",
    "\n",
    "To create the web service, run the following command (update the account name with your storage account and container names):\n",
    "\n",
    "```\n",
    "az ml service create batch -f batch_score.py --in=--trained-model:food_inspection.model --in=--input-data:https://<yourStorageAccount>.blob.core.windows.net/<containerName>/food_inspections2.csv --out=--output-data -v -n samplebatch\n",
    "```\n",
    "\n",
    "Once the web service is successfully created, use the following command to run the job. The output is stored remotely using the wasb protocol: wasb[s]://&lt;containername>@&lt;accountname>.blob.core.windows.net/&lt;path>.\n",
    "\n",
    "```\n",
    "az ml service run batch --out=--output-data:wasbs://<containerName>@<StorageAccountName>.blob.core.windows.net/output.parquet -v -n samplebatch \n",
    "```\n",
    "\n",
    "#### View a list jobs running against your web service\n",
    "\n",
    "View the list of jobs running against your web service to get the ID of the job:\n",
    "\n",
    "```\n",
    "az ml service listjobs batch -n batchwebservice\n",
    "```\n",
    "\n",
    "Use the Job Name to view the status with the following command:\n",
    "\n",
    "```\n",
    "az ml service viewjob batch -n batchwebservice -j <paste job name here>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Spark - python",
   "language": "python",
   "name": "spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
