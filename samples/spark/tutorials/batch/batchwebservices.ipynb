{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How to create a batch web service for a Spark model on Azure\n",
    "\n",
    "Before running the tutorial, you must configure your DSVM as specified in the README on the [Machine Learing Operationaliztion](https://aka.ms/o16ncli) GitHub repo. If you have previously configured your DSVM, you may want to check the GitHub repo to ensure that you are using the most recent instructions.\n",
    "\n",
    "In the tutorial you will use [Apache Spark](http://spark.apache.org/) to create a model that uses a Logistic Regression learner to predict food inspection results. To do this, you will call the Spark Python API ([PySpark](http://spark.apache.org/docs/0.9.0/python-programming-guide.html)) to load a dataset, train a model using the dataset, and publish a batch scoring API for the model.\n",
    "\n",
    "You then use the Azure CLI to operationalize the model and to call the web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data\n",
    "\n",
    "The tutorial uses the *Food Inspections Data Set* which contains the results of food inspections that were conducted in Chicago. To facilitate this tutorial, we have placed a copy of the data in the ```azureml/datasets``` folder. The original dataset is available from the [City of Chicago data portal](https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import the relevant PySpark bindings\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Parse the food inspections dataset and create numerical labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inspections = spark.read.csv(\"datasets/food_inspections1.csv\",mode='DROPMALFORMED',inferSchema=False)\n",
    "\n",
    "schema = StructType([StructField(\"id\", IntegerType(), False), \n",
    "                     StructField(\"name\", StringType(), False), \n",
    "                     StructField(\"results\", StringType(), False), \n",
    "                     StructField(\"violations\", StringType(), True)])\n",
    "\n",
    "df = sqlContext.createDataFrame(inspections.rdd.map(lambda l: (int(l[0]), l[1], l[12], l[13] if l[13] else '')), schema) \n",
    "df.registerTempTable('CountResults')\n",
    "\n",
    "def labelForResults(s):\n",
    "    if s == 'Fail':\n",
    "        return 0.0\n",
    "    elif s == 'Pass w/ Conditions' or s == 'Pass':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return -1.0\n",
    "    \n",
    "label = UserDefinedFunction(labelForResults, DoubleType())\n",
    "labeledData = df.select(label(df.results).alias('label'), df.violations).where('label >= 0')\n",
    "labeledData.write.format('parquet').mode('overwrite').save('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create and save the model\n",
    "Next, train a logistic regression model to predict inspection results. The following code tokenizes each \"violations\" string to get the individual words in each string. It then uses a HashingTF to convert each set of tokens into a feature vector which is passed to the logistic regression algorithm to construct a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(labeledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, you save the model to use when deploying the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"food_inspection.model\")\n",
    "print \"Model saved\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a Batch Web Service\n",
    "\n",
    "In this section, you will create and deploy a batch webservice that will make predictions on given data using the model that you trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create a prediction script \n",
    "\n",
    "Your goal is to create a web service that you can call to make predictions based on the input data. To create a web service using the model you saved, you start by authoring a script to do the scoring.\n",
    "\n",
    "In the provided sample, the script takes a data file as its input-data argument, uses the model specified by the user as model input, and makes predictions on the data by running the model. The script then saves the predictions as a parquet file to the path provided through the output-data argument.\n",
    "\n",
    "The save file call (%%save_file -f batch_score.py) in the first line of the of the cell saves the contents of the cell to a local file with the name supplied by the -f argument.\n",
    "\n",
    "When calling the batch web service create command for the scoring script using the AML CLI, you must provide the parameters that you identified in the script as command line arguments.\n",
    "\n",
    "You can choose to parameterize your scoring script files per your descretion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%save_file -f batch_score.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "import argparse\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext.getOrCreate(sc)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input-data\")\n",
    "parser.add_argument(\"--trained-model\")\n",
    "parser.add_argument(\"--output-data\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "print str(args.input_data)\n",
    "print str(args.trained_model)\n",
    "print str(args.output_data)\n",
    "\n",
    "testdata = spark.read.csv(str(args.input_data),mode='DROPMALFORMED',inferSchema=False)\n",
    "\n",
    "schema = StructType([StructField(\"id\", IntegerType(), False),\n",
    "                     StructField(\"name\", StringType(), False),\n",
    "                     StructField(\"results\", StringType(), False),\n",
    "                     StructField(\"violations\", StringType(), True)])\n",
    "\n",
    "testDf = sqlContext.createDataFrame(testdata.rdd.map(lambda l: (int(l[0]), l[1], l[12], l[13] if l[13] else '')), schema).where(\"results = 'Fail' OR results = 'Pass' OR results = 'Pass w/ Conditions'\")\n",
    "\n",
    "model = PipelineModel.load(args.trained_model)\n",
    "\n",
    "predictionsDf = model.transform(testDf)\n",
    "predictionsDf.write.format(\"parquet\").mode('overwrite').save(str(args.output_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Use the CLI to deploy and manage your batch web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can deploy an operationalized model as a web service locally and to a cluster.\n",
    "\n",
    "Open an SSH session to your DSVM and change to the folder notebooks/azureml/batch.\n",
    "\n",
    "```\n",
    "cd ~/notebooks/azureml/batch\n",
    "```\n",
    "\n",
    "#### Deploy to local machine\n",
    "\n",
    "To create the batch web service locally on the DSVM, set your CLI environment to run in local mode.\n",
    "\n",
    "```\n",
    "az ml env local\n",
    "```\n",
    "\n",
    "The following command creates a web service local to the DSVM. It reads the model from local storage and specifies that the output is written to local storage.\n",
    "\n",
    "```\n",
    "az ml service create batch -f batch_score.py -n batchwebservice --in=--input-data --in=--trained-model:food_inspection.model --out=--output-data:food_inspection_predictions.parquet\n",
    "```\n",
    "\n",
    "Once the web service is successfully created, the following command runs a job against the web service:\n",
    "\n",
    "```\n",
    "az ml service run batch -n samplebatch --out=--output-data:output.parquet -w\n",
    "```\n",
    "\n",
    "#### Deploy to a cluster\n",
    "\n",
    "In the following example, the input data is stored remotely. When you create the batch web service, you can use data that is stored in a private blob in Azure storage. When using this scenario, the blob must be in the storage account that was created during your environment setup and that setup must be the active one in your az ml CLI environment. When you create the service the CLI uses the credentials stored in your environment to retrieve the data.\n",
    "\n",
    "Download the *food_inspections2.csv* data file and upload it to a container in storage account that was created when you set up your environment.\n",
    "\n",
    "To find the name of the storage account, open your *.amlenvrc* file and find the AML_STORAGE_ACCT_NAME variable.\n",
    "\n",
    "To create the batch web service locally on the DSVM, set your CLI environment to run in local mode.\n",
    "```\n",
    "az ml env cluster\n",
    "```\n",
    "\n",
    "To create the web service, run the following command (update the account name with your storage account and container names):\n",
    "\n",
    "```\n",
    "az ml service create batch -f batch_score.py --in=--trained-model:food_inspection.model --in=--input-data:https://<yourStorageAccount>.blob.core.windows.net/<containerName>/food_inspections2.csv --out=--output-data -v -n samplebatch\n",
    "```\n",
    "\n",
    "Once the web service is successfully created, use the following command to run the job. The output is stored remotely using the wasb protocol: wasb[s]://&lt;containername>@&lt;accountname>.blob.core.windows.net/&lt;path>.\n",
    "\n",
    "```\n",
    "az ml service run batch --out=--output-data:wasbs://<containerName>@<StorageAccountName>.blob.core.windows.net/output.parquet -v -n samplebatch \n",
    "```\n",
    "\n",
    "#### View a list jobs running against your web service\n",
    "\n",
    "View the list of jobs running against your web service to get the ID of the job:\n",
    "\n",
    "```\n",
    "az ml service listjobs batch -n batchwebservice\n",
    "```\n",
    "\n",
    "Use the Job Name to view the status with the following command:\n",
    "\n",
    "```\n",
    "az ml service viewjob batch -n batchwebservice -j <paste job name here>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 Spark - local",
   "language": "python",
   "name": "spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
