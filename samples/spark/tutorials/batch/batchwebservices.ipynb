{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How to create a batch web service for a Spark model on Azure\n",
    "\n",
    "Before running the tutorial, you must configure your DSVM as specified in the README on the [Machine Learing Operationaliztion](https://aka.ms/o16ncli) GitHub repo. If you have previously configured your DSVM, you may want to check the GitHub repo to ensure that you are using the most recent instructions.\n",
    "\n",
    "\n",
    "In the tutorial you will use [Apache Spark](http://spark.apache.org/) to create a model that uses a Logistic Regression learner to predict food inspection results. To do this, you will call the Spark Python API ([PySpark](http://spark.apache.org/docs/0.9.0/python-programming-guide.html)) to load a dataset, train a model using the dataset, and publish a batch scoring API for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data\n",
    "\n",
    "The tutorial uses the *Food Inspections Data Set* which contains the results of food inspections that were conducted in Chicago. To facilitate this tutorial, we have placed a copy of the data in the ```azureml/datasets``` folder. The original dataset is available from the [City of Chicago data portal](https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import the relevant PySpark bindings\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Parse the food inspections dataset and create numerical labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inspections = spark.read.csv(\"../datasets/food_inspections1.csv\",mode='DROPMALFORMED',inferSchema=False)\n",
    "\n",
    "schema = StructType([StructField(\"id\", IntegerType(), False), \n",
    "                     StructField(\"name\", StringType(), False), \n",
    "                     StructField(\"results\", StringType(), False), \n",
    "                     StructField(\"violations\", StringType(), True)])\n",
    "\n",
    "df = sqlContext.createDataFrame(inspections.rdd.map(lambda l: (int(l[0]), l[1], l[12], l[13] if l[13] else '')), schema) \n",
    "df.registerTempTable('CountResults')\n",
    "\n",
    "def labelForResults(s):\n",
    "    if s == 'Fail':\n",
    "        return 0.0\n",
    "    elif s == 'Pass w/ Conditions' or s == 'Pass':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return -1.0\n",
    "    \n",
    "label = UserDefinedFunction(labelForResults, DoubleType())\n",
    "labeledData = df.select(label(df.results).alias('label'), df.violations).where('label >= 0')\n",
    "labeledData.write.format('parquet').mode('overwrite').save('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create and save the model\n",
    "Next, you train a logistic regression model to predict inspection results. The following code tokenizes each \"violations\" string to get the individual words in each string. It then uses a HashingTF to convert each set of tokens into a feature vector which is passed to the logistic regression algorithm to construct a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(labeledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, you save the model to use when deploying the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"food_inspection.model\")\n",
    "print \"Model saved\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a Batch Web Service\n",
    "\n",
    "In this section, you will create and deploy a batch webservice that will make predictions on given data using the model that you trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create a prediction script \n",
    "\n",
    "Your goal is to create a web service that you can call to make predictions based on the input data. To create a web service using the model you saved, you start by authoring a script to do the scoring (see the sample script called batch_score.py in the same folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Use the CLI to deploy and manage your batch web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Deploy to local machine\n",
    "\n",
    "To create the batch web service locally on the DSVM, set your CLI environment to run in local mode.\n",
    "```\n",
    "az ml env local\n",
    "```\n",
    "\n",
    "To create the web service, run the following command (update the account name with your storage account and container names):\n",
    "\n",
    "```\n",
    "az ml service create batch -f batch_score.py --in=--trained-model:food_inspection.model --in=--input-data:https://<yourStorageAccount>.blob.core.windows.net/<containerName>/food_inspections2.csv --out=--output-data -v -n samplebatch\n",
    "```\n",
    "\n",
    "Once the web service is successfully created, use the following command to run the job. Note that the wasbs path (wasb[s]://<containername>@<accountname>.blob.core.windows.net/<path>) for the output file.\n",
    "\n",
    "```\n",
    "az ml service run batch --out=--output-data:wasbs://<containerName>@<accountName>.blob.core.windows.net/output.parquet -v -n samplebatch \n",
    "```\n",
    "\n",
    "View the list of jobs running against your web service to get the ID of the job:\n",
    "\n",
    "```\n",
    "az ml service listjobs batch -n batchwebservice\n",
    "```\n",
    "Use the Job Name to view the status with the following command:\n",
    "```\n",
    "az ml service viewjob batch -n batchwebservice -j <paste job name here>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 Spark - local",
   "language": "python",
   "name": "spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
